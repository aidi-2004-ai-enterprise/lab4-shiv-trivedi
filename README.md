# Lab 4 â€” Bankruptcy Prediction (README) ğŸ¢ğŸ’°

**Author:** Shivkumar Trivedi 
**Lab 4:** Bankruptcy Prediction through Binary Classification ğŸ“ŠğŸ”  

---

## ğŸ“¦ What's in this folder

- `assets/` â€” folder containing charts and visualizations ğŸ“Š
- `data.csv` â€” 6,819 companies with ~96 financial features ğŸ“ˆ
- `dataset-snapshot.py` â€” quick scan: shape, class balance, extreme-value flags + two display-only charts ğŸ”
- `lab4-report.md` â€” my graded answers to the decision areas (WHY-focused) ğŸ“
- `psi_results.csv` â€” PSI numbers exported during my run ğŸ“„
- `PSI-analysis.py` â€” simple PSI check (train vs test) with one bar chart ğŸ“Š
- `README.md` â€” this file ğŸ“–
- `requirements.txt` â€” Python dependencies for the project ğŸ

---

## ğŸ¯ What I investigated

- ğŸ¤– Picking a baseline + two stronger models  
- ğŸ”§ How to prep data for each model (keep it minimal)  
- âš–ï¸ What to do about the tiny positive class  
- ğŸ“Š Outliers vs. real distress signals  
- ğŸ¯ Train/test representativeness (PSI)  
- ğŸ“ Metrics that match rare-event reality  
- ğŸ” Interpretability that business can follow

---

## ğŸ” Things the data made obvious

### âš ï¸ Severe imbalance
- **220 bankrupt** vs **6,599 non-bankrupt** â†’ ~**3.2%** positive (~**1:29** ratio) ğŸ“‰
- Accuracy alone is useless here; recall and calibrated probabilities matter âš–ï¸

### ğŸš¨ A few "impossible" values
- **13 features** with absurd maximums (e.g., **Current Ratio ~ 2.75B**, **Revenue/Share ~ 3.02B**) ğŸ’¸  
- These look like entry errors; most other ratios sit in sensible ranges âœ…

### âœ… Train/test looks like the same population
- PSI across tested features was **< 0.1** (no meaningful drift) ğŸ“Š  
- Stratified sampling preserved the base rate ğŸ¯

---

## âœ… Decisions (driven by evidence)

### ğŸ¤– Model Selection
**Models:** Logistic Regression (baseline), Random Forest, XGBoost  
**Why:** Baseline is explainable; trees/boosting capture thresholds & interactions common in financial ratios

### ğŸ”§ Pre-processing Strategy  
**Approach:** Scale **only** for Logistic Regression; same simple pipeline for fairness  
**Why:** Trees don't need scaling; consistent prep keeps comparisons honest

### âš–ï¸ Imbalance Handling
**Strategy:** Stratified splits; class weighting; SMOTE only if needed (within train folds)  
**Why:** Raise recall without fabricating too much noise

### ğŸ“Š Outlier Treatment
**Approach:** Cap only obviously wrong values; keep legitimate extremes  
**Why:** Distress looks extremeâ€”don't delete the signal we care about

### ğŸ¯ Sampling Bias Assessment
**Method:** PSI train vs test and monitor over time  
**Why:** Validate today's split and catch drift before performance drops

### ğŸ“ Evaluation Metrics
**Strategy:** Use **predict_proba**; primary **PR-AUC**; also ROC-AUC, F1; check calibration  
**Why:** Imbalanced reality + decisions use probabilities, not just labels

### ğŸ” Explainability Approach
**Methods:** SHAP for the best tree model; coefficients for the baseline  
**Why:** Clear, case-level reasons for stakeholders and regulators

---

## ğŸ“¸ Screens I captured for the report

- ğŸ“Š Class distribution bar chart (shows the ~1:29 split)  
- ğŸš¨ "Implausible extremes â€” top features" bar chart  
- âœ… PSI table + PSI bar chart (all green / below 0.1)  
- ğŸ’» Terminal snippets: dataset shape and flagged features

---

## ğŸ§  Evidence â†’ Action (quick map)

| Evidence from analysis ğŸ”            | What it told me ğŸ’¡                     | Action I took âš¡                            |
|--------------------------------------|-----------------------------------------|----------------------------------------------|
| ~1:29 class ratio                    | Rare positives; accuracy is misleading  | Stratified CV; class weights; SMOTE if needed|
| 13 absurd max values                 | Entry errors, not finance reality       | Cap only those; keep real extremes           |
| PSI < 0.1 (train vs test)            | Split is representative                 | Proceed with stratified K-fold               |
| Ratios mostly in sensible ranges     | Minimal prep is enough                  | Scale LR only; trees on raw ratios           |

---

## ğŸ“ Key Learning Outcomes

### ğŸ“Š Data Understanding
- Learned to identify and handle severe class imbalance in financial data
- Developed skills in detecting data quality issues vs. legitimate extreme values
- Gained experience with Population Stability Index (PSI) for drift detection

### ğŸ¤– Model Strategy
- Understood when to use different preprocessing approaches for different model types
- Learned the importance of starting simple before adding complexity
- Developed intuition for model selection based on problem characteristics

### ğŸ“ Evaluation Insights
- Recognized why accuracy is misleading for imbalanced datasets
- Learned to prioritize probability-based metrics over simple classification
- Understood the business need for model explainability in financial contexts

---

## ğŸš€ Future Improvements

### ğŸ”„ Model Enhancement
- Experiment with ensemble methods combining all three models
- Test advanced sampling techniques (ADASYN, BorderlineSMOTE)
- Explore threshold optimization techniques for cost-sensitive learning

### ğŸ“Š Data Engineering
- Implement automated outlier detection pipelines
- Develop real-time PSI monitoring for production deployment
- Create feature engineering based on domain expertise

### ğŸ­ Production Readiness
- Build model monitoring dashboard for performance tracking
- Implement A/B testing framework for model comparison
- Develop automated retraining pipelines based on drift detection

---

## ğŸ™‹ Final note

I kept the pipeline small ğŸ”§, the "why" front-and-center ğŸ’¡, and the visuals as proof I actually ran the analysis ğŸ“Š. If something changes (new data or drift), PSI and calibration checks are my early warning system âš ï¸.

**Remember:** In bankruptcy prediction, it's better to be cautious and flag potential risks than to miss actual bankruptcies! ğŸ›¡ï¸ğŸ’¼